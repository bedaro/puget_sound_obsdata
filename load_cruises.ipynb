{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ffeb960-a370-439d-838c-73289c631ca3",
   "metadata": {},
   "source": [
    "# load_cruises\n",
    "\n",
    "This notebook extracts, transforms, and loads data from NANOOS-hosted Salish cruises (such as the UW PRISM cruises). Most of this data can be downloaded from http://nvs.nanoos.org/CruiseSalish.\n",
    "\n",
    "Each downloaded file is a .zip file which contains at least one .csv file; typically, there is one downcast file and one file with both upcast and lab data. The format of this data changed around 2017, and this notebook is able to detect and handle both formats. Come of the cruises from the mid-2010s do not have the lab/bottle data included in the downloads on the website; I was able to request this missing data from NANOOS and it was supplied in NetCDF format. If you receive the same files, they can be placed in the directory `data/prism/netcdf/` and this notebook will process them to look for any data from cruises where it was unable to find lab data in the .csv's.\n",
    "\n",
    "While there are common station names for most of the Salish cruises, they are not always in perfectly consistent locations. This may matter depending on your application of the data, so the locations are renamed with alphabetic letter suffixes (for instance, there may be P11, P11a, P11b, ..., P11z, P11aa, P11bb, ...) to ensure uniqueness even if the coordinates for these stations are very close together.\n",
    "\n",
    "Each cruise is assigned to the database as an independent entry in the `sources` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22cc147d-110d-4b14-b4b7-e1120d5d1385",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_files = \"data/prism/Salish_Cruise-*.zip\"\n",
    "bottle_cdfs = \"data/prism/netcdf/*_bottle.nc\"\n",
    "\n",
    "import glob\n",
    "import zipfile\n",
    "import re\n",
    "import uuid\n",
    "import csv\n",
    "import string\n",
    "from io import TextIOWrapper\n",
    "from multiprocessing import Pool\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "from netCDF4 import Dataset\n",
    "import db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa800d37-f3eb-4021-b3fd-52e86236efdf",
   "metadata": {},
   "source": [
    "Station location is interwoven into the CSVs, and its processing cannot be parallelized because there are so many (but not all) duplicates. Read through all the zip files to get the station names/locations and cruise IDs first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00b86b7a-784b-4431-9154-5a0d44cc3b4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geom</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P1</th>\n",
       "      <td>POINT (551839.304 5318404.955)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P28</th>\n",
       "      <td>POINT (540908.380 5283990.478)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P3</th>\n",
       "      <td>POINT (537914.065 5328503.637)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P4</th>\n",
       "      <td>POINT (533200.284 5343294.734)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P5</th>\n",
       "      <td>POINT (547312.002 5303619.781)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                geom\n",
       "name                                \n",
       "P1    POINT (551839.304 5318404.955)\n",
       "P28   POINT (540908.380 5283990.478)\n",
       "P3    POINT (537914.065 5328503.637)\n",
       "P4    POINT (533200.284 5343294.734)\n",
       "P5    POINT (547312.002 5303619.781)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downcast_re = re.compile('SalishCruise_.*_downcast\\\\.csv$')\n",
    "\n",
    "# A converter helper to strip whitespace from the ends of strings\n",
    "stripper = lambda s: s.strip()\n",
    "\n",
    "# Parse a latitude/longitude value given in degrees and minutes with a cardinal direction\n",
    "# into a positive or negative decimal degree value\n",
    "def to_decdeg(degmin):\n",
    "    (deg, minute, plusminus) = [t(s) for t,s in zip((int, float, str), degmin.split())]\n",
    "    return (deg + minute / 60) * (-1 if plusminus in ('S','W') else 1)\n",
    "\n",
    "location_gdfs = []\n",
    "source_dfs = []\n",
    "for f in glob.glob(zip_files):\n",
    "    with zipfile.ZipFile(f) as zf:\n",
    "        files = zf.namelist()\n",
    "        # Find the file in this zip that matches the downcast filename pattern\n",
    "        # See https://stackoverflow.com/a/19502692\n",
    "        downcast_file = next(filter(lambda s: s if downcast_re.search(s) else None, files))\n",
    "        with zf.open(downcast_file) as f:\n",
    "            # See https://anitagraser.com/2019/01/23/from-csv-to-geodataframe-in-two-lines/\n",
    "            try:\n",
    "                df = pd.read_csv(f, skiprows = lambda x: x in [1,2], skipinitialspace=True,\n",
    "                                 encoding=\"ISO-8859-1\", converters={ 'Cruise ID': stripper, 'Station': stripper },\n",
    "                                 usecols=['Cruise ID','Station','Latitude Deg','Longitude Deg'], na_values='None')\n",
    "                df = df.loc[df['Station'] != 'None']\n",
    "                df.columns = ['cruise','y','x','name']\n",
    "            except ValueError:\n",
    "                f.seek(0)\n",
    "                # New format from 2018 onwards. Pandas has a hard time with the header so ignore\n",
    "                # it\n",
    "                df = pd.read_csv(f, skiprows=2, sep=',', encoding=\"ISO-8859-1\", index_col=False, na_values='None',\n",
    "                                 header=0, names=['Upload time','lat', 'lon', 'time', 'station','cruise'],\n",
    "                                 usecols=['station','lat','lon','cruise'],\n",
    "                                 converters={ 'lat': to_decdeg, 'lon': to_decdeg, 'station': stripper, 'cruise': stripper })\n",
    "                df.columns = ['y','x','name','cruise']\n",
    "            df.dropna(subset='name', inplace=True)\n",
    "            # Make a GeoDataFrame for the stations\n",
    "            gdf = gpd.GeoDataFrame(\n",
    "                df.drop(['cruise','x','y'],axis=1),\n",
    "                crs='epsg:6318',\n",
    "                geometry=[Point(xy) for xy in zip(df.x, df.y)])\n",
    "            location_gdfs.append(gdf)\n",
    "            source_dfs.append(pd.DataFrame(df['cruise'].drop_duplicates()))\n",
    "# Preserve the different locations across the cruises. Drop duplicates on geometry, then append letter(s)\n",
    "# to the duplicate names so they're unique. The first one doesn't get any suffix, then the next 26 get a-z, then\n",
    "# the next 26 get aa-zz\n",
    "locations = gpd.GeoDataFrame(pd.concat(location_gdfs, ignore_index=True).drop_duplicates(subset=['geometry']))\n",
    "full_seq = ([\"\"] + list(string.ascii_lowercase) + (\n",
    "             pd.Series(list(string.ascii_lowercase)) + pd.Series(list(string.ascii_lowercase))\n",
    "            ).tolist())\n",
    "for n,group in locations.groupby(\"name\"):\n",
    "    locations.loc[locations[\"name\"] == n, \"name\"] += full_seq[:len(group)]\n",
    "locations = locations.set_index('name').rename_geometry('geom').to_crs(epsg=32610)\n",
    "locations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9d7a90-a282-4af6-b6f8-a099fda81c2e",
   "metadata": {},
   "source": [
    "Assemble a DataFrame of all the cruise sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72337894-f8c7-407c-a8d8-8da943d1e764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study</th>\n",
       "      <th>agency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RC0007</td>\n",
       "      <td>Salish Cruise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AQ201610</td>\n",
       "      <td>Salish Cruise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CB1019</td>\n",
       "      <td>Salish Cruise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TN087</td>\n",
       "      <td>Salish Cruise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AQ201710</td>\n",
       "      <td>Salish Cruise</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      study         agency\n",
       "0    RC0007  Salish Cruise\n",
       "1  AQ201610  Salish Cruise\n",
       "2    CB1019  Salish Cruise\n",
       "3     TN087  Salish Cruise\n",
       "4  AQ201710  Salish Cruise"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sources = pd.concat(source_dfs, ignore_index=True).drop_duplicates()\n",
    "# Rename the column\n",
    "sources.columns = ['study']\n",
    "# Add an \"agency\" column for all the cruises\n",
    "sources['agency'] = \"Salish Cruise\"\n",
    "sources.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc02087-92b0-460d-805a-e341ed169955",
   "metadata": {},
   "source": [
    "Save the cruise sources and locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbbb2e73-fb01-427f-b8c1-1ff1fcd4ba4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agency</th>\n",
       "      <th>study</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Salish Cruise</td>\n",
       "      <td>RC0007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Salish Cruise</td>\n",
       "      <td>AQ201610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Salish Cruise</td>\n",
       "      <td>CB1019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Salish Cruise</td>\n",
       "      <td>TN087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Salish Cruise</td>\n",
       "      <td>AQ201710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           agency     study\n",
       "id                         \n",
       "2   Salish Cruise    RC0007\n",
       "3   Salish Cruise  AQ201610\n",
       "4   Salish Cruise    CB1019\n",
       "5   Salish Cruise     TN087\n",
       "6   Salish Cruise  AQ201710"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine = db.connect()\n",
    "locations.to_postgis('stations', con=engine, schema='obsdata', index=True, index_label='name', if_exists='append')\n",
    "sources.to_sql('sources', con=engine, schema='obsdata', index=False, if_exists='append')\n",
    "# Re-fetch sources so we have the right foreign key values for observations\n",
    "sources = pd.read_sql(\"SELECT * FROM obsdata.sources WHERE agency='Salish Cruise'\", con=engine, index_col='id')\n",
    "sources.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ff9491-784d-4df5-a892-5819242c53af",
   "metadata": {},
   "source": [
    "Process all the .zip files containing downcast or lab data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "569427f3-bbde-4a3f-93ba-182bd29eecb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RC0007       True\n",
       "AQ201610    False\n",
       "CB1019      False\n",
       "TN087        True\n",
       "AQ201710     True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labupcast_csv_re = re.compile('SalishCruise_.*_labupcast\\\\.csv$')\n",
    "labupcast_excel_re = re.compile('SalishCruise_.*_labupcast\\\\.xlsx$')\n",
    "\n",
    "# Per-thread database connection\n",
    "def db_init():\n",
    "    global thread_con\n",
    "    thread_con = db.connect()\n",
    "\n",
    "# Iterate over all the zip files\n",
    "def process_file(zip_filename):\n",
    "    bottle_found = False\n",
    "    with zipfile.ZipFile(zip_filename) as zf:\n",
    "        files = zf.namelist()\n",
    "        # Process the downcast data\n",
    "        # Find the file in this zip that matches the downcast filename pattern\n",
    "        # See https://stackoverflow.com/a/19502692\n",
    "        downcast_file = next(filter(lambda s: s if downcast_re.search(s) else None, files))\n",
    "        with zf.open(downcast_file) as f:\n",
    "            # Read the header with a csv.reader to figure out which format it is\n",
    "            rdr = csv.reader(TextIOWrapper(f, encoding=\"ISO-8859-1\"))\n",
    "            header = next(rdr)\n",
    "            f.seek(0)\n",
    "            if \"Cruise ID\" in header:\n",
    "                df = pd.read_csv(f, skiprows = lambda x: x in [1,2], skipinitialspace=True,\n",
    "                                 encoding=\"ISO-8859-1\", parse_dates=['UTC Time'],\n",
    "                                 converters={ 'Cruise ID': stripper, 'Station': stripper })\n",
    "                df = df.loc[df['Station'] != 'None']\n",
    "                df = df.rename(columns={\n",
    "                    'Cruise ID': 'cruise',\n",
    "                    'UTC Time': 'time',\n",
    "                    'Station': 'station',\n",
    "                    'Depth': 'depth',\n",
    "                    'Temperature': 'temp',\n",
    "                    'Salinity': 'salt',\n",
    "                    'Oxygen Concentration MG': 'o2',\n",
    "                    'Chlorophyll Fluorescence ': 'chla'\n",
    "                })\n",
    "            else:\n",
    "                # New format from ~2017 onwards\n",
    "                df = pd.read_csv(f, skiprows=lambda r: r == 1, sep=',', encoding=\"ISO-8859-1\", index_col=False,\n",
    "                                 parse_dates=['NMEAtimeUTC'], skipinitialspace=True,\n",
    "                                 converters={ 'Station': stripper, 'CruiseID': stripper })\n",
    "                df = df.rename(columns={\n",
    "                    'NMEAtimeUTC': 'time',\n",
    "                    'Station': 'station',\n",
    "                    'CruiseID': 'cruise',\n",
    "                    'Cast': 'cast_id',\n",
    "                    'flECO-AFL: Fluorescence  WET Labs ECO-AFL/FL ': 'chla',\n",
    "                    'ph: pH': 'ph',\n",
    "                    't090C: Temperature ': 'temp',\n",
    "                    'depSM: Depth ': 'depth',\n",
    "                    'sbeox0Mg/L: Oxygen  SBE 43 ': 'o2',\n",
    "                    'sal00: Salinity  Practical ': 'salt'\n",
    "                })\n",
    "                    \n",
    "            df.dropna(subset='station', inplace=True)\n",
    "\n",
    "            # Assign cast UUID's\n",
    "            if 'cast_id' in df.columns:\n",
    "                # We already have cast identifiers, replace them with UUID's\n",
    "                df.loc[:, 'cast_id'] = df.groupby(['station', 'cast_id'])['cast_id'].transform(lambda g: uuid.uuid4())\n",
    "            else:\n",
    "                # No casts have been defined, so estimate with Grouper\n",
    "                df['cast_id'] = 1\n",
    "                df.loc[:, 'cast_id'] = df.groupby(['station', pd.Grouper(key='time', freq='30min')])['cast_id'].transform(lambda g: uuid.uuid4())\n",
    "\n",
    "            # Eliminate any duplicate depth observations by taking the mean of all the\n",
    "            # duplicated measurements\n",
    "            df = df.groupby(['station','cruise','time','cast_id','depth']).mean(numeric_only=True).reset_index()\n",
    "\n",
    "            # Set timezone\n",
    "            df['time'].dt = df['time'].dt.tz_localize('UTC')\n",
    "\n",
    "            # Assign source IDs\n",
    "            df = df.merge(sources.reset_index(), left_on='cruise', right_on='study', copy=False)\n",
    "\n",
    "            # Each spreadsheet has different column names depending on the type of\n",
    "            # data that was given.\n",
    "            for param in ['temp','salt','o2','chla','ph']:\n",
    "                if param not in df.columns:\n",
    "                    continue\n",
    "                # For each data column, make a view that drops the NaNs, then append\n",
    "                # that view's station, time, depth, cast, and column value to\n",
    "                # outs. Create a filled parameter_id that corresponds to the\n",
    "                # data column and append it as well.\n",
    "                view = df.dropna(subset=param)\n",
    "                \n",
    "                processed_data = pd.DataFrame({\n",
    "                    'source_id': view['id'],\n",
    "                    'datetime': view['time'],\n",
    "                    'depth': view['depth'],\n",
    "                    'value': view[param],\n",
    "                    'location_id': view['station'],\n",
    "                    'cast_id': view['cast_id']\n",
    "                })\n",
    "                processed_data['parameter_id'] = param\n",
    "                # Remove any cast IDs for parameters that were measured fewer than 5 times\n",
    "                # (these cannot be interpolated reliably)\n",
    "                counts = processed_data[['cast_id','parameter_id','value']].groupby(['cast_id','parameter_id']).count()\n",
    "                m = processed_data.merge(counts, how='left', left_on=('cast_id','parameter_id'), right_index=True)\n",
    "                processed_data.loc[m['value_y'] < 5, 'cast_id'] = np.nan\n",
    "\n",
    "                processed_data.to_sql('observations', con=thread_con, schema='obsdata',\n",
    "                                      index=False, if_exists='append')\n",
    "\n",
    "        # If there is a lab/upcast file, extract the lab data\n",
    "        found_format = None\n",
    "        for fmt,rxp in (('csv', labupcast_csv_re), ('excel', labupcast_excel_re)):\n",
    "            # Very python-ic way to see if there's a matching file for the given\n",
    "            # regexp. Stop after the first match since there isn't supposed to be\n",
    "            # more than one\n",
    "            for f in filter(lambda s: s if rxp.search(s) else None, files):\n",
    "                labupcast_file = f\n",
    "                found_format = fmt\n",
    "                break\n",
    "        if found_format is not None:\n",
    "            bottle_found = True\n",
    "            with zf.open(labupcast_file) as f:\n",
    "                if found_format == 'csv':\n",
    "                    lu_df = pd.read_csv(f, skiprows = lambda x: x in [1,2], skipinitialspace=True,\n",
    "                                        encoding=\"ISO-8859-1\", parse_dates=['UTC Time'],\n",
    "                                        converters={ 'Cruise ID': stripper, 'Station': stripper })\n",
    "                    lu_df = lu_df.loc[lu_df['Station'] != 'None']\n",
    "                    lu_df = lu_df.rename(columns={\n",
    "                        'Cruise ID': 'cruise',\n",
    "                        'UTC Time': 'time',\n",
    "                        'Station': 'station',\n",
    "                        'Depth': 'depth',\n",
    "                        'Oxygen Concentration MG Titration': 'o2',\n",
    "                        'Chlorophyll Concetration': 'chla',\n",
    "                        'Nitrate': 'no3',\n",
    "                        'Nitrite': 'no2',\n",
    "                        'Ammonium': 'nh4',\n",
    "                        'Phosphate': 'po4',\n",
    "                        'Silicate': 'sioh4'\n",
    "                    })\n",
    "                    # Set timezone\n",
    "                    lu_df['time'].dt = lu_df['time'].dt.tz_localize('UTC')\n",
    "                else:\n",
    "                    lu_df = pd.read_excel(f)\n",
    "                    # Merge date and time into a single column\n",
    "                    # At least one of the files has invalid UTC time values because an offset was\n",
    "                    # naively added to produce \"times\" that look like 24:56:00. So work from the\n",
    "                    # local time columns instead\n",
    "                    lu_df['time'] = pd.to_datetime(lu_df[\"DATE_LOCAL\"].astype(str) + \" \" + lu_df[\"TIME_LOCAL\"].astype(str))\n",
    "                    lu_df['time'] = lu_df['time'].dt.tz_localize(\"US/Pacific\")\n",
    "                    lu_df['time'] = lu_df['time'].dt.tz_convert('UTC')\n",
    "\n",
    "                    # Put stations in consistent format\n",
    "                    lu_df['station'] = \"P\" + lu_df['STATION_NO'].astype(str)\n",
    "                    lu_df = lu_df.rename(columns={\n",
    "                        'CRUISE_ID': 'cruise',\n",
    "                        'DEPTH (M)': 'depth',\n",
    "                        'OXYGEN_avg_mg_L': 'o2',\n",
    "                        'NITRATE_UMOL_L': 'no3',\n",
    "                        'NITRITE_UMOL_L': 'no2',\n",
    "                        'AMMONIUM_UMOL_L': 'nh4',\n",
    "                        'PHOSPHATE_UMOL_L': 'po4',\n",
    "                        'SILICATE_UMOL_L': 'sioh4'\n",
    "                    })\n",
    "                    # set o2 values to NaN if OXYGEN_FLAG_W is not 2\n",
    "                    lu_df.loc[lu_df['OXYGEN_FLAG_W'] != 2, 'o2'] = np.nan\n",
    "                    # set all nutrient values to NaN if NUTRIENTS_FLAG_W is not 2\n",
    "                    lu_df.loc[lu_df['NUTRIENTS_FLAG_W'] != 2, ['no3','no2','nh4','po4','sioh4']] = np.nan\n",
    "\n",
    "                # Eliminate any duplicate depth observations by taking the mean of all the\n",
    "                # duplicated measurements\n",
    "                lu_df = lu_df.groupby(['station','cruise','time','depth']).mean(numeric_only=True).reset_index()\n",
    "\n",
    "                # Prevent unique constraint violations from lab vs CTD oxygen/chla values by adding a minute\n",
    "                # to the dates. At least some labupcast files use identical collection times for the bottles\n",
    "                # as for the downcast.\n",
    "                lu_df['time'] += pd.to_timedelta(1, 'min')\n",
    "                \n",
    "                # Assign source IDs\n",
    "                lu_df = lu_df.merge(sources.reset_index(), left_on='cruise', right_on='study', copy=False)\n",
    "\n",
    "                # Each spreadsheet has different column names depending on the type of\n",
    "                # data that was given.\n",
    "                for param in ['o2','chla','no3','no2','nh4','po4','sioh4']:\n",
    "                    if param not in lu_df.columns:\n",
    "                        continue\n",
    "                    # For each data column, make a view that drops the NaNs, then append\n",
    "                    # that view's station, time, depth, and column value to\n",
    "                    # outs. Create a filled parameter_id that corresponds to the\n",
    "                    # data column and append it as well.\n",
    "                    view = lu_df.dropna(subset=param)\n",
    "\n",
    "                    processed_data = pd.DataFrame({\n",
    "                        'source_id': view['id'],\n",
    "                        'datetime': view['time'],\n",
    "                        'depth': view['depth'],\n",
    "                        'value': view[param],\n",
    "                        'location_id': view['station']\n",
    "                    })\n",
    "                    processed_data['parameter_id'] = param\n",
    "                    processed_data.to_sql('observations', con=thread_con, schema='obsdata',\n",
    "                                          index=False, if_exists='append')\n",
    "        # If there is no lab/upcast file, we'll take care of it when reading the netcdf files\n",
    "    return (df['cruise'][0], bottle_found)\n",
    "\n",
    "with Pool(initializer=db_init) as p:\n",
    "    results = p.map(process_file, glob.glob(zip_files))\n",
    "cruises, bottles = zip(*results)\n",
    "results_series = pd.Series(bottles, cruises)\n",
    "results_series.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b60c25-a10b-491a-8024-7d6cadf1cd10",
   "metadata": {},
   "source": [
    "Assemble a list of all the cruise IDs for which the task threads did not find bottle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c34d2769-8d5d-40b3-a3c9-8e93f6317523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['AQ201610', 'CB1019', 'CB1023', 'TN315', 'TN301', 'TN296', 'CB1050',\n",
       "       'CB1041', 'CB1045', 'TN322', 'CB1075', 'MV1403', 'RBTSN2017', 'CB1065',\n",
       "       'TN281', 'SH1604', 'CB1028', 'CAB771', 'CB1041', 'CB1034', 'TN333'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_bottle_cruises = results_series.loc[~results_series].index\n",
    "missing_bottle_cruises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5a48a4-d044-4abd-8ae2-49b56f0f3397",
   "metadata": {},
   "source": [
    "Process NetCDF files which match the cruise IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8aa1a997-95c3-4b2b-afee-0b8bfd9a548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdftime2pdtime(times):\n",
    "    # Convert the times in days since year 0 to pandas Timestamps. This requires some creativity to\n",
    "    # avoid an OverflowError\n",
    "    return pd.Timestamp('1/1/2000') + pd.to_timedelta(times - 730486, unit='D')\n",
    "\n",
    "# Now process NetCDF bottle data for missing years\n",
    "def process_cdf(cdf):\n",
    "    ds = Dataset(cdf, \"r\")\n",
    "    cruiseid = ds.cruise.strip()\n",
    "    if cruiseid not in missing_bottle_cruises:\n",
    "        # We already have this cruise\n",
    "        return\n",
    "    # Stations are integers. Leave them be for now...\n",
    "    data = pd.DataFrame({\n",
    "        \"location_id\": ds['station'][:],\n",
    "        \"datetime\": cdftime2pdtime(ds['time_utc'][:]),\n",
    "        \"depth\": ds['depth'][:],\n",
    "        \"o2\": ds['oxygen_bottle'][:],\n",
    "        \"nh4\": ds['ammonium_bottle'][:],\n",
    "        \"no3\": ds['nitrate_bottle'][:],\n",
    "        \"no2\": ds['nitrite_bottle'][:],\n",
    "        \"po4\": ds['phosphate_bottle'][:],\n",
    "        \"sioh4\": ds['silicate_bottle'][:],\n",
    "        \"chla\": ds['chlorophyll_bottle'][:]\n",
    "    }).dropna(thresh=4)\n",
    "    ds.close()\n",
    "    # drop data with invalid station numbers\n",
    "    data = data.loc[data['location_id'] > 0]\n",
    "    # Now we can fix the station names\n",
    "    data[\"location_id\"] = \"P\" + data[\"location_id\"].astype(str)\n",
    "    # Add source ID for the cruise\n",
    "    data['source_id'] = sources.loc[sources['study'] == cruiseid].index[0]\n",
    "    # Prevent unique constraint violations from lab vs CTD oxygen/chla values by adding a minute\n",
    "    # to the dates. At least some labupcast files use identical collection times for the bottles\n",
    "    # as for the downcast.\n",
    "    data['datetime'] += pd.to_timedelta(1, 'min')\n",
    "    \n",
    "    for param in ['o2','chla','no3','no2','nh4','po4','sioh4']:\n",
    "        # For each data column, make a view that drops the NaNs, then append\n",
    "        # that view's station, time, depth, and column value to\n",
    "        # outs. Create a filled parameter_id that corresponds to the\n",
    "        # data column and append it as well.\n",
    "        view = data.dropna(subset=param)\n",
    "\n",
    "        processed_data = pd.DataFrame({\n",
    "            'source_id': view['source_id'],\n",
    "            'datetime': view['datetime'],\n",
    "            'depth': view['depth'],\n",
    "            'value': view[param],\n",
    "            'location_id': view['location_id']\n",
    "        })\n",
    "        processed_data['parameter_id'] = param\n",
    "        processed_data.to_sql('observations', con=thread_con, schema='obsdata',\n",
    "                              index=False, if_exists='append')\n",
    "\n",
    "with Pool(initializer=db_init) as p:\n",
    "    p.map(process_cdf, glob.glob(bottle_cdfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2ce8c1-fba9-440a-a25a-34971077798a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:puget_sound_obs_data]",
   "language": "python",
   "name": "conda-env-puget_sound_obs_data-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
