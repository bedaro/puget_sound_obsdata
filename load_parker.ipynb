{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d6fd1bc-3f84-42a6-b036-83afc3e9c973",
   "metadata": {},
   "source": [
    "# load_parker\n",
    "\n",
    "A notebook for loading observational data preprocessed using [LiveOcean's \"obs\"](https://github.com/parkermac/LO/tree/main/obs) format.\n",
    "\n",
    "This notebook does not handle the mooring data because I already have a working processing system for the ORCA buoys.\n",
    "\n",
    "Parameters for tuning:\n",
    "\n",
    " * *parker_obs_path*: put outputs from obs here.\n",
    " * *data_included*: a collection of subfolder names under *parker_obs_path* to read and load into the database. As of 2024, including all years of the data below results in over 85 million observation rows.\n",
    " * *info_files*: a glob matching pattern for the info pickle files, used for populating the stations table.\n",
    " * *data_files*: a glob matching pattern for the individual data files, typically named with a 4-digit year.\n",
    " * *loc_tolerance*: The obs data does not group locations together beyond the cast level, so this notebook performs location deduplication by grouping stations within this distance (m) of an existing station. The larger this value is, the less spatial precision there will be in where the observation was taken; the smaller it is, the fewer unique stations there will be in the database. `dfo1` is the worst offender at creating many stations. Using a 100 m tolerance with the default *data_included* results in over 47 thousand individual stations.\n",
    "\n",
    "On my setup with a separate database server on a gigabit ethernet connection, loading all of the default data took over an hour to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "c1c80f68-9b99-4f0a-987f-a4ac43d0020a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parker_obs_path = 'data/parker'\n",
    "data_included = ('collias','dfo1','ecology_nc','nceiCoastal','nceiPNW','nceiSalish','LineP','WOD','ocnms_ctd')\n",
    "\n",
    "info_files = '**/info_*.p'\n",
    "data_files = '**/????.p'\n",
    "\n",
    "loc_tolerance = 100\n",
    "\n",
    "import glob\n",
    "import uuid\n",
    "import random\n",
    "from pathlib import Path\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from sqlalchemy.exc import IntegrityError\n",
    "\n",
    "import db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e20935-d7e3-41a1-a3d8-fa88b4d1b038",
   "metadata": {},
   "source": [
    "Cast UUID's are generated by this notebook in a way that guarantees they will be repeatable between invocations, but always unique to a particular cid of a particular source and year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "38fd4b59-06a3-40fc-b1b7-a9baa27af0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rd = random.Random()\n",
    "rd.seed(5) # Should be unique to this ETL notebook\n",
    "baseuuid = uuid.UUID(int=rd.getrandbits(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3820b1-5a0d-4b29-a90a-6514ac7dfd44",
   "metadata": {},
   "source": [
    "An example of what the obs format looks like when unpickled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e9e63cb-255b-4d2b-a03d-79201b207448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/parker/collias/bottle/1961.p\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cid</th>\n",
       "      <th>cruise</th>\n",
       "      <th>time</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>name</th>\n",
       "      <th>z</th>\n",
       "      <th>CT</th>\n",
       "      <th>SA</th>\n",
       "      <th>DO (uM)</th>\n",
       "      <th>NO2 (uM)</th>\n",
       "      <th>SiO4 (uM)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>1961-04-18 18:30:00</td>\n",
       "      <td>47.934814</td>\n",
       "      <td>-122.634599</td>\n",
       "      <td>HCE501</td>\n",
       "      <td>-96</td>\n",
       "      <td>8.851958</td>\n",
       "      <td>29.602401</td>\n",
       "      <td>262.762941</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>1961-04-18 18:30:00</td>\n",
       "      <td>47.934814</td>\n",
       "      <td>-122.634599</td>\n",
       "      <td>HCE501</td>\n",
       "      <td>-72</td>\n",
       "      <td>8.864435</td>\n",
       "      <td>29.601889</td>\n",
       "      <td>263.388567</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>1961-04-18 18:30:00</td>\n",
       "      <td>47.934814</td>\n",
       "      <td>-122.634599</td>\n",
       "      <td>HCE501</td>\n",
       "      <td>-48</td>\n",
       "      <td>8.961322</td>\n",
       "      <td>29.360403</td>\n",
       "      <td>269.332015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>1961-04-18 18:30:00</td>\n",
       "      <td>47.934814</td>\n",
       "      <td>-122.634599</td>\n",
       "      <td>HCE501</td>\n",
       "      <td>-29</td>\n",
       "      <td>9.046389</td>\n",
       "      <td>29.199680</td>\n",
       "      <td>274.649836</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>1961-04-18 18:30:00</td>\n",
       "      <td>47.934814</td>\n",
       "      <td>-122.634599</td>\n",
       "      <td>HCE501</td>\n",
       "      <td>-19</td>\n",
       "      <td>9.120350</td>\n",
       "      <td>29.049315</td>\n",
       "      <td>280.906097</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cid cruise                time        lat         lon    name   z  \\\n",
       "0    0   None 1961-04-18 18:30:00  47.934814 -122.634599  HCE501 -96   \n",
       "1    0   None 1961-04-18 18:30:00  47.934814 -122.634599  HCE501 -72   \n",
       "2    0   None 1961-04-18 18:30:00  47.934814 -122.634599  HCE501 -48   \n",
       "3    0   None 1961-04-18 18:30:00  47.934814 -122.634599  HCE501 -29   \n",
       "4    0   None 1961-04-18 18:30:00  47.934814 -122.634599  HCE501 -19   \n",
       "\n",
       "         CT         SA     DO (uM)  NO2 (uM)  SiO4 (uM)  \n",
       "0  8.851958  29.602401  262.762941       NaN        NaN  \n",
       "1  8.864435  29.601889  263.388567       NaN        NaN  \n",
       "2  8.961322  29.360403  269.332015       NaN        NaN  \n",
       "3  9.046389  29.199680  274.649836       NaN        NaN  \n",
       "4  9.120350  29.049315  280.906097       NaN        NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "root = Path(parker_obs_path)\n",
    "for src in data_included:\n",
    "    src_root = root / src\n",
    "    for i in src_root.glob(data_files):\n",
    "        print(i)\n",
    "        data_df = pd.read_pickle(i)\n",
    "        display(data_df.head())\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9210c173-cadb-40f5-be64-cd3163a88771",
   "metadata": {},
   "source": [
    "## Compile and Aggregate Stations\n",
    "\n",
    "First, deduplicate station names by assembling a look-up DF mapping name+year to assigned DB name. Start by reading all the info files into a giant GDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "a0f09581-ec80-4070-aa72-6e1a2bcfafa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cid</th>\n",
       "      <th>time</th>\n",
       "      <th>name</th>\n",
       "      <th>cruise</th>\n",
       "      <th>src</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1952-02-22 10:38:00</td>\n",
       "      <td>HCE501</td>\n",
       "      <td>None</td>\n",
       "      <td>collias</td>\n",
       "      <td>POINT (527291.582 5309119.74)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1952-06-04 22:25:00</td>\n",
       "      <td>HCE501</td>\n",
       "      <td>None</td>\n",
       "      <td>collias</td>\n",
       "      <td>POINT (527291.582 5309119.74)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1952-06-05 19:02:00</td>\n",
       "      <td>HCE501</td>\n",
       "      <td>None</td>\n",
       "      <td>collias</td>\n",
       "      <td>POINT (527291.582 5309119.74)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1952-07-31 16:11:00</td>\n",
       "      <td>HCE501</td>\n",
       "      <td>None</td>\n",
       "      <td>collias</td>\n",
       "      <td>POINT (527291.582 5309119.74)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1952-10-13 19:05:00</td>\n",
       "      <td>HCE501</td>\n",
       "      <td>None</td>\n",
       "      <td>collias</td>\n",
       "      <td>POINT (527291.582 5309119.74)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cid                time    name cruise      src  \\\n",
       "0    0 1952-02-22 10:38:00  HCE501   None  collias   \n",
       "1    1 1952-06-04 22:25:00  HCE501   None  collias   \n",
       "2    2 1952-06-05 19:02:00  HCE501   None  collias   \n",
       "3    3 1952-07-31 16:11:00  HCE501   None  collias   \n",
       "4    4 1952-10-13 19:05:00  HCE501   None  collias   \n",
       "\n",
       "                        geometry  \n",
       "0  POINT (527291.582 5309119.74)  \n",
       "1  POINT (527291.582 5309119.74)  \n",
       "2  POINT (527291.582 5309119.74)  \n",
       "3  POINT (527291.582 5309119.74)  \n",
       "4  POINT (527291.582 5309119.74)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "info_dfs = []\n",
    "for src in data_included:\n",
    "    src_root = root / src\n",
    "    # Some locations aren't named in some sources, so keep track of the source name so we can generate some names\n",
    "    newdfs = [pd.read_pickle(i) for i in src_root.glob(info_files)]\n",
    "    for df in newdfs:\n",
    "        df['src'] = src\n",
    "    info_dfs.extend(newdfs)\n",
    "\n",
    "info_df = pd.concat(info_dfs, ignore_index=True)\n",
    "# FIXME no documentation on what the lat/lon datum is or if it's been standardized. I assume it's WGS84\n",
    "info_gdf = gpd.GeoDataFrame({'cid': info_df.index, 'time': info_df['time'], 'name': info_df['name'],\n",
    "                             'cruise': info_df['cruise'], 'src': info_df['src']},\n",
    "                            geometry=[Point(xy) for xy in zip(info_df['lon'], info_df['lat'])],\n",
    "                            crs='epsg:4326').to_crs(epsg=32610)\n",
    "info_gdf['time'] = pd.to_datetime(info_gdf['time'])\n",
    "display(info_gdf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf2b076-6123-44b0-8918-33803f9248a4",
   "metadata": {},
   "source": [
    "Now do a spatial group for everything within a small radius (*loc_tolerance* above). Each entry within the group must be given a unique name, so if 'name' alone is not unique, append an incrementing number to it. Keep track of final names in the lookup DF, and assemble the locations table in the DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1850a8c7-0c73-4f80-ab69-96e1c0631230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>HCE501</th>\n",
       "      <td>POINT (527291.582 5309119.74)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADM201</th>\n",
       "      <td>POINT (516506.266 5335940.591)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADM204</th>\n",
       "      <td>POINT (527862.368 5319496.902)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADM202</th>\n",
       "      <td>POINT (523215.387 5331517.772)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PSB301</th>\n",
       "      <td>POINT (539020.178 5305297.273)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              geometry\n",
       "name                                  \n",
       "HCE501   POINT (527291.582 5309119.74)\n",
       "ADM201  POINT (516506.266 5335940.591)\n",
       "ADM204  POINT (527862.368 5319496.902)\n",
       "ADM202  POINT (523215.387 5331517.772)\n",
       "PSB301  POINT (539020.178 5305297.273)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_simple = info_gdf.copy()\n",
    "info_simple['year'] = info_simple['time'].dt.year\n",
    "cruise_idx = ~info_simple['cruise'].isna()\n",
    "info_simple['crname'] = info_simple['name'].str.strip()\n",
    "info_simple.loc[cruise_idx, 'crname'] = info_simple.loc[cruise_idx].apply(\n",
    "    lambda r: str(r['crname']) + '_' + str(r['cruise']).strip(), axis=1)\n",
    "\n",
    "def gen_name(base, existing):\n",
    "    i = 1\n",
    "    while f\"{base}_{i}\" in existing:\n",
    "        i += 1\n",
    "    return f\"{base}_{i}\"\n",
    "\n",
    "master = None # The master list of unique stations as a GeoSeries\n",
    "# The raw data that will be used for an internal lookup table later\n",
    "lookup = {'name': [], 'cruise': [], 'year': [], 'geometry': [], 'dbname': []}\n",
    "for i,row in info_simple.drop_duplicates(subset=('name','year','geometry')).iterrows():\n",
    "    new = False\n",
    "    if master is None:\n",
    "        newname = row['crname']\n",
    "        master = gpd.GeoSeries([row['geometry']], index=[newname], crs=info_simple.crs)\n",
    "        master.index.name = 'name'\n",
    "    else:\n",
    "        nearby = master.dwithin(row['geometry'], loc_tolerance)\n",
    "        if np.any(nearby):\n",
    "            # Already have a nearby location, so use it\n",
    "            newname = master.loc[nearby].sort_values().index[0]\n",
    "            #print(f\"{row['crname']}: Found existing nearby station {newname}\")\n",
    "        else:\n",
    "            if row['crname'] is None:\n",
    "                # Nothing nearby, and no name given so generate one\n",
    "                newname = gen_name(row['src'], master.index)\n",
    "            elif row['crname'] in master.index:\n",
    "                # Nothing nearby, but the name given is already taken\n",
    "                newname = gen_name(row['crname'], master.index)\n",
    "            else:\n",
    "                # We can safely use the name as-is\n",
    "                newname = row['crname']\n",
    "            master.loc[newname] = row['geometry']\n",
    "\n",
    "    lookup['name'].append(row['name'])\n",
    "    lookup['cruise'].append(row['cruise'])\n",
    "    lookup['year'].append(row['year'])\n",
    "    lookup['geometry'].append(row['geometry'])\n",
    "    lookup['dbname'].append(newname)\n",
    "\n",
    "master_gdf = gpd.GeoDataFrame(geometry=master, crs=info_simple.crs)\n",
    "lookup_df = pd.DataFrame(lookup)\n",
    "#lookup_df.set_index(['name','cruise','year','geometry'], inplace=True)\n",
    "master_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da64ebba-7e02-4b36-8812-8d0386140544",
   "metadata": {},
   "source": [
    "Our lookup DF allows relating the combination of name, cruise, year, and coordinates to the newly generated station name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "41c38bdf-76a0-4f1c-8cfb-e573c2e62b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>cruise</th>\n",
       "      <th>year</th>\n",
       "      <th>geometry</th>\n",
       "      <th>dbname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HCE501</td>\n",
       "      <td>None</td>\n",
       "      <td>1952</td>\n",
       "      <td>POINT (527291.58150689 5309119.739996711)</td>\n",
       "      <td>HCE501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADM201</td>\n",
       "      <td>None</td>\n",
       "      <td>1952</td>\n",
       "      <td>POINT (516506.2662512224 5335940.591206845)</td>\n",
       "      <td>ADM201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADM204</td>\n",
       "      <td>None</td>\n",
       "      <td>1952</td>\n",
       "      <td>POINT (527862.3682839083 5319496.902014364)</td>\n",
       "      <td>ADM204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADM202</td>\n",
       "      <td>None</td>\n",
       "      <td>1952</td>\n",
       "      <td>POINT (523215.387201385 5331517.77196983)</td>\n",
       "      <td>ADM202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PSB301</td>\n",
       "      <td>None</td>\n",
       "      <td>1952</td>\n",
       "      <td>POINT (539020.1779876386 5305297.272531977)</td>\n",
       "      <td>PSB301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name cruise  year                                     geometry  dbname\n",
       "0  HCE501   None  1952    POINT (527291.58150689 5309119.739996711)  HCE501\n",
       "1  ADM201   None  1952  POINT (516506.2662512224 5335940.591206845)  ADM201\n",
       "2  ADM204   None  1952  POINT (527862.3682839083 5319496.902014364)  ADM204\n",
       "3  ADM202   None  1952    POINT (523215.387201385 5331517.77196983)  ADM202\n",
       "4  PSB301   None  1952  POINT (539020.1779876386 5305297.272531977)  PSB301"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d1cf6e-0852-45e2-9613-9915a5f06090",
   "metadata": {},
   "source": [
    "## Load Sources, Stations\n",
    "\n",
    "Define a unique source for each entry in *data_included* but indicate that it comes from Parker's obs outputs be setting the agency field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f045296-003b-493c-8f44-93d49f756bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1ca52c2b-17e8-4308-807d-1ea66fffff30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'collias': 4,\n",
       " 'dfo1': 5,\n",
       " 'ecology_nc': 6,\n",
       " 'nceiCoastal': 7,\n",
       " 'nceiPNW': 8,\n",
       " 'nceiSalish': 9,\n",
       " 'LineP': 10,\n",
       " 'WOD': 11,\n",
       " 'ocnms_ctd': 12}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_ids = {}\n",
    "df = pd.read_sql_table(\"sources\", con=engine, schema='obsdata', index_col='id')\n",
    "for src in data_included:\n",
    "    agency = 'Parker'\n",
    "    study = f'{src} dataset'\n",
    "    source_row = df.loc[(df['agency'] == agency) & (df['study'] == study)]\n",
    "    if len(source_row) == 0:\n",
    "        df = pd.DataFrame({\n",
    "            \"agency\": [agency],\n",
    "            \"study\": [study]\n",
    "        })\n",
    "        df.to_sql('sources', con=engine, schema='obsdata', index=False, if_exists='append')\n",
    "\n",
    "        # Refresh the sources so we can fetch the primary key\n",
    "        df = pd.read_sql_table(\"sources\", con=engine, schema='obsdata', index_col='id')\n",
    "        source_row = df.loc[(df['agency'] == agency) & (df['study'] == study)]\n",
    "    \n",
    "    source_ids[src] = source_row.index[0]\n",
    "source_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2e1df62b-8aa9-41f2-9590-125b1cf73dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_gdf.rename_geometry('geom').to_postgis('stations', con=engine,\n",
    "                                              schema='obsdata', index=True,\n",
    "                                              index_label='name', if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e057782-3eef-4f66-b406-5555344bd5db",
   "metadata": {},
   "source": [
    "## Load Observations\n",
    "\n",
    "This is the meat of the notebook. Define the function that processes all the observations from a single file. It takes optional `con` and `start_col` parameters so it can be called manually to diagnose/retry any that fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "f0471094-5e02-4ea5-b46f-1091711fb20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_param_map = {\n",
    "    'CT': 'temp',\n",
    "    'SA': 'salt',\n",
    "    'DO (uM)': 'o2',\n",
    "    'NO3 (uM)': 'no3',\n",
    "    'NO2 (uM)': 'no2',\n",
    "    'NH4 (uM)': 'nh4',\n",
    "    'PO4 (uM)': 'po4',\n",
    "    'SiO4 (uM)': 'sioh4',\n",
    "    'Chl (mg m-3)': 'chla',\n",
    "    'DIC (uM)': None,\n",
    "    'TA (uM)': None\n",
    "}\n",
    "\n",
    "col_factors = {\n",
    "    'DO (uM)': 32/1000\n",
    "}\n",
    "\n",
    "def process_data(pth, src, con=None, start_col=None):\n",
    "    if con is None:\n",
    "        con = thread_con\n",
    "    data = pd.read_pickle(pth)\n",
    "    # Sometimes there are true duplicate entries, so merge them\n",
    "    data = data.groupby(['cid','lon','lat','time','z','name','cruise'], dropna=False).agg('mean').reset_index()\n",
    "    data[['name','cruise']] = data[['name','cruise']].replace(np.nan, None)\n",
    "    data.dropna(subset=('z','time','lon','lat'), inplace=True)\n",
    "\n",
    "    data_gdf = gpd.GeoDataFrame(data, geometry=[Point(xy) for xy in zip(data['lon'],data['lat'])],\n",
    "                                crs='epsg:4326').to_crs(epsg=32610)\n",
    "    data_gdf['time'] = pd.to_datetime(data['time'])\n",
    "    data_gdf['year'] = data_gdf['time'].dt.year\n",
    "    # Some files don't use an object dtype for cruise and this breaks the below merge, so force it\n",
    "    data_gdf['cruise'] = data_gdf['cruise'].astype(object)\n",
    "    # FIXME not using indices for lookup_df probably slows this down, but converting to\n",
    "    # a MultiIndex messes up the dtype on cruise and makes the merge fail\n",
    "    m = data_gdf.merge(lookup_df, left_on=('name','cruise','year','geometry'), #right_index=True)\n",
    "                       right_on=('name','cruise','year','geometry'))\n",
    "    dest = pd.DataFrame({\n",
    "        # Assuming dataset has already been corrected for daylight savings and is in PST\n",
    "        'datetime': pd.DatetimeIndex(m['time']).tz_localize(-8*3600),\n",
    "        'depth': -m['z'],\n",
    "        'location_id': m['dbname']\n",
    "    })\n",
    "    dest['source_id'] = source_ids[src]\n",
    "    dest['cast_id'] = m.groupby('cid')['cid'].transform(lambda g: uuid.uuid3(baseuuid, f'{pth.relative_to(root).parent}_{pth.stem}_{g}'))\n",
    "    # create copies with parameter_id/value pairs\n",
    "    found_start = start_col is None\n",
    "    for c in set(data.columns) - {'cid','lon','lat','name','time','cruise','z'}:\n",
    "        found_start |= c == start_col\n",
    "        if not found_start:\n",
    "            continue\n",
    "        pid = col_param_map[c]\n",
    "        if pid is None:\n",
    "            continue\n",
    "        dest['parameter_id'] = pid\n",
    "        dest['value'] = data[c] * col_factors[c] if c in col_factors else data[c]\n",
    "        try:\n",
    "            dest.dropna(subset=('value',)).to_sql('observations', con=con, schema='obsdata', index=False, if_exists='append')\n",
    "        except IntegrityError:\n",
    "            return (False,c)\n",
    "    return (True,None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c0545e-6711-41e9-b8af-985a4af51ab1",
   "metadata": {},
   "source": [
    "Improve performance somewhat by processing in parallel, but no more than 4 task threads because it would otherwise overwhelm any database. This could take an hour or more to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "f96f8b58-bc15-4a64-b685-f9a0520a5b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_init():\n",
    "    global thread_con\n",
    "    thread_con = db.connect()\n",
    "\n",
    "with Pool(4, initializer=db_init) as p:\n",
    "    inputs = []\n",
    "    for src in data_included:\n",
    "        for d in (root / src).glob(data_files):\n",
    "            inputs.append((d, src))\n",
    "    successes = p.starmap(process_data, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160e131a-3800-4f6d-bafa-697f038ca227",
   "metadata": {},
   "source": [
    "`inputs` is a list of tuples containing a data file path and source ID.\n",
    "\n",
    "`successes` is a list of tuples indicating whether the matching input was successfully committed in full, and if not the name of the column where the failure occurred. So if there are any entries here, troubleshooting those failed data files is required, but the above cell will not have to be run again since they can be loaded piecemeal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "0d5b8db3-108a-41a1-b869-2dedfcb6ece1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [0, 1]\n",
       "Index: []"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(successes, index=pd.MultiIndex.from_tuples(inputs))\n",
    "results_df.loc[~results_df[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4adcaf3-c440-4fb6-ad12-57f182a96271",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:puget_sound_obs_data]",
   "language": "python",
   "name": "conda-env-puget_sound_obs_data-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
