{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be45cadd-df85-4d6d-a79b-76be63d31283",
   "metadata": {},
   "source": [
    "# load_ecology_bottle\n",
    "\n",
    "This notebook extracts, transforms, and loads Ecology lab/bottle data from their Marine Water study. This data is not available for direct download and instead must be retrieved through a public records request.\n",
    "\n",
    "From the data I have reviewed, the format is not entirely consistent in how the spreadsheets are laid out but there is enough of a common pattern that this notebook can handle at least a few cases. Your mileage may vary, unfortunately.\n",
    "\n",
    "The format I have for this data is as .zip files that contain one or more Excel workbooks. If yours are the same format, just place the .zip files in the directory `data/ecology/bottle` and this notebook will find them.\n",
    "\n",
    "This notebook only imports data that has fully passed QA: the QA column for each observation must contain \"2_0_3\". See Ecology's documentation, which should be included with the files, to understand what this means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2ab8a47-3c5c-4a3b-9751-9d6f6e601c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecology_bottle_files = \"data/ecology/bottle/*.zip\"\n",
    "\n",
    "import glob\n",
    "import zipfile\n",
    "import re\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "from sqlalchemy import text\n",
    "from openpyxl import load_workbook\n",
    "import numpy as np\n",
    "import db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fa81d3-b4ae-400c-b4e1-cb5b5b63106f",
   "metadata": {},
   "source": [
    "Construct a DataFrame that maps between the database parameter IDs, the corresponding Ecology parameter column names, and a multiplier for unit conversions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a093e137-a606-4e83-88d1-8d45d0562094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_name</th>\n",
       "      <th>key_name</th>\n",
       "      <th>conv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chla_Lab</td>\n",
       "      <td>chla</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chla_Lab (ug/L)</td>\n",
       "      <td>chla</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NH4_Lab</td>\n",
       "      <td>nh4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NH4(uM)D</td>\n",
       "      <td>nh4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NO3_Lab</td>\n",
       "      <td>no3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NO3(uM)D</td>\n",
       "      <td>no3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NO2_Lab</td>\n",
       "      <td>no2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NO2(uM)D</td>\n",
       "      <td>no2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PO4_Lab</td>\n",
       "      <td>po4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PO4(uM)D</td>\n",
       "      <td>po4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SiOH4_Lab</td>\n",
       "      <td>sioh4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SiOH4(uM)D</td>\n",
       "      <td>sioh4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           raw_name key_name  conv\n",
       "0          Chla_Lab     chla     1\n",
       "1   Chla_Lab (ug/L)     chla     1\n",
       "2           NH4_Lab      nh4     1\n",
       "3          NH4(uM)D      nh4     1\n",
       "4           NO3_Lab      no3     1\n",
       "5          NO3(uM)D      no3     1\n",
       "6           NO2_Lab      no2     1\n",
       "7          NO2(uM)D      no2     1\n",
       "8           PO4_Lab      po4     1\n",
       "9          PO4(uM)D      po4     1\n",
       "10        SiOH4_Lab    sioh4     1\n",
       "11       SiOH4(uM)D    sioh4     1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecology_column_names = pd.DataFrame({\n",
    "    \"raw_name\": [\"Chla_Lab\",\"Chla_Lab (ug/L)\",\"NH4_Lab\",\"NH4(uM)D\",\n",
    "                 \"NO3_Lab\",\"NO3(uM)D\",\"NO2_Lab\",\"NO2(uM)D\",\"PO4_Lab\",\n",
    "                 \"PO4(uM)D\",\"SiOH4_Lab\",\"SiOH4(uM)D\"],\n",
    "    \"key_name\": [\"chla\",\"chla\",\"nh4\",\"nh4\",\"no3\",\"no3\",\"no2\",\"no2\",\n",
    "                 \"po4\",\"po4\",\"sioh4\",\"sioh4\"],\n",
    "    \"conv\": [1,1,1,1,1,1,1,1,1,1,1,1]\n",
    "})\n",
    "ecology_column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da52eaf1-6fc3-47fe-a0b9-db15adfb9506",
   "metadata": {},
   "source": [
    "Fetch the Ecology source ID, which was created in [load_ecology_ctd](load_ecology_ctd.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0d10e58-f2b8-42d9-b2ad-ab7f414c1ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "engine = db.connect()\n",
    "# Refresh the sources so we can fetch the primary key\n",
    "df = pd.read_sql_table(\"sources\", con=engine, schema='obsdata', index_col='id')\n",
    "ecology_source_id = df.loc[(df['agency'] == \"WA Ecology\") & (df['study'] == \"MarineWater\")].index[0]\n",
    "print(ecology_source_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270e83f8-d8f6-4c1f-9f8e-46e82332c7a5",
   "metadata": {},
   "source": [
    "Process all the spreadsheets found in .zip files in the target directory.\n",
    "\n",
    "Individual Excel files can be vastly different sizes, so it makes sense to use a producer/consumer multiprocessing model here with a Queue. Unfortunately we need to apply deduplication before commiting to the DB, so a Manager is used with a server process to gather all the result data. A lock is shared between the workers since the column list update process is not thread-safe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd03f41e-cb6f-4cbe-9512-614219549826",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1391364/1490424298.py:14: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  yield pd.read_excel(data_f, sheet_name=sheet, parse_dates=[2], na_values=-9999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KingCounty_Apr2018DataRequest/KingCounty_JEMSChlaData_Aug2018.xlsx\n",
      "KingCounty_Apr2018DataRequest/KingCounty_JEMSData_Apr2018.xlsx\n",
      "KingCounty_Apr2018DataRequest/KingCounty_MainBasinChla&NutData_Jul2018.xlsx\n",
      "KingCounty_Apr2018DataRequest/KingCounty_MainBasinData_Apr2018.xlsx\n",
      "KingCounty_Apr2018DataRequest/KingCounty_QMHarborData_Apr2018.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1391364/1490424298.py:47: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '<DatetimeArray>\n",
      "['2001-02-26 00:00:00', '2001-03-20 00:00:00', '2001-03-20 00:00:00',\n",
      " '2001-04-26 00:00:00', '2001-05-21 00:00:00', '2001-05-21 00:00:00',\n",
      " '2001-06-19 00:00:00', '2001-06-19 00:00:00', '2001-07-18 00:00:00',\n",
      " '2001-07-18 00:00:00', '2001-08-20 00:00:00', '2001-08-20 00:00:00',\n",
      " '2001-11-27 00:00:00', '2001-12-28 00:00:00', '2004-02-19 00:00:00',\n",
      " '2004-02-19 00:00:00', '2004-03-10 00:00:00', '2004-03-10 00:00:00',\n",
      " '2004-04-06 00:00:00', '2004-04-06 00:00:00', '2004-05-03 00:00:00',\n",
      " '2004-05-03 00:00:00', '2004-06-01 00:00:00', '2004-06-01 00:00:00',\n",
      " '2004-08-10 00:00:00', '2004-08-10 00:00:00', '2004-09-21 00:00:00',\n",
      " '2004-09-21 00:00:00', '2004-10-26 00:00:00', '2004-10-26 00:00:00',\n",
      " '2010-01-14 00:00:00', '2010-01-14 00:00:00', '2010-02-10 00:00:00',\n",
      " '2010-02-10 00:00:00', '2010-03-02 00:00:00', '2010-04-05 00:00:00',\n",
      " '2010-05-05 00:00:00', '2010-05-05 00:00:00', '2010-06-01 00:00:00',\n",
      " '2010-07-06 00:00:00', '2010-07-06 00:00:00', '2010-08-10 00:00:00',\n",
      " '2010-09-13 00:00:00', '2010-10-04 00:00:00', '2010-10-04 00:00:00',\n",
      " '2010-11-02 00:00:00', '2010-11-02 00:00:00', '2010-12-07 00:00:00']\n",
      "Length: 48, dtype: datetime64[ns]' has dtype incompatible with datetime64[ns, UTC], please explicitly cast to a compatible dtype first.\n",
      "  df['datetime'].fillna(df['Date'], inplace=True)\n",
      "/tmp/ipykernel_1391364/1490424298.py:47: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '<DatetimeArray>\n",
      "['1999-09-02 00:00:00', '1999-09-02 00:00:00', '1999-09-02 00:00:00',\n",
      " '1999-09-02 00:00:00', '1999-10-15 00:00:00', '1999-10-15 00:00:00',\n",
      " '1999-10-15 00:00:00', '1999-10-15 00:00:00', '1999-11-23 00:00:00',\n",
      " '1999-11-23 00:00:00',\n",
      " ...\n",
      " '2017-11-10 00:00:00', '2017-11-10 00:00:00', '2017-11-10 00:00:00',\n",
      " '2017-11-10 00:00:00', '2017-11-10 00:00:00', '2017-12-06 00:00:00',\n",
      " '2017-12-06 00:00:00', '2017-12-06 00:00:00', '2017-12-06 00:00:00',\n",
      " '2017-12-06 00:00:00']\n",
      "Length: 2650, dtype: datetime64[ns]' has dtype incompatible with datetime64[ns, UTC], please explicitly cast to a compatible dtype first.\n",
      "  df['datetime'].fillna(df['Date'], inplace=True)\n",
      "/tmp/ipykernel_1391364/1490424298.py:47: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '<DatetimeArray>\n",
      "['1999-07-08 00:00:00', '1999-07-08 00:00:00', '1999-07-08 00:00:00',\n",
      " '1999-07-08 00:00:00', '1999-07-08 00:00:00', '1999-07-08 00:00:00',\n",
      " '1999-07-08 00:00:00', '1999-07-08 00:00:00', '1999-07-08 00:00:00',\n",
      " '1999-07-08 00:00:00',\n",
      " ...\n",
      " '2017-12-11 00:00:00', '2017-12-11 00:00:00', '2017-12-11 00:00:00',\n",
      " '2017-12-11 00:00:00', '2017-12-11 00:00:00', '2017-12-11 00:00:00',\n",
      " '2017-12-11 00:00:00', '2017-12-11 00:00:00', '2017-12-11 00:00:00',\n",
      " '2017-12-11 00:00:00']\n",
      "Length: 3816, dtype: datetime64[ns]' has dtype incompatible with datetime64[ns, UTC], please explicitly cast to a compatible dtype first.\n",
      "  df['datetime'].fillna(df['Date'], inplace=True)\n",
      "/tmp/ipykernel_1391364/1490424298.py:47: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '<DatetimeArray>\n",
      "['1999-02-20 00:00:00', '1999-03-15 00:00:00', '1999-03-15 00:00:00',\n",
      " '1999-03-15 00:00:00', '1999-03-15 00:00:00', '1999-03-15 00:00:00',\n",
      " '1999-03-15 00:00:00', '1999-03-18 00:00:00', '1999-03-18 00:00:00',\n",
      " '1999-03-18 00:00:00',\n",
      " ...\n",
      " '2017-12-11 00:00:00', '2017-12-11 00:00:00', '2017-12-11 00:00:00',\n",
      " '2017-12-11 00:00:00', '2017-12-11 00:00:00', '2017-12-11 00:00:00',\n",
      " '2017-12-11 00:00:00', '2017-12-11 00:00:00', '2017-12-11 00:00:00',\n",
      " '2017-12-11 00:00:00']\n",
      "Length: 4797, dtype: datetime64[ns]' has dtype incompatible with datetime64[ns, UTC], please explicitly cast to a compatible dtype first.\n",
      "  df['datetime'].fillna(df['Date'], inplace=True)\n",
      "/tmp/ipykernel_1391364/1490424298.py:47: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '<DatetimeArray>\n",
      "['1999-09-02 00:00:00', '1999-09-02 00:00:00', '1999-09-02 00:00:00',\n",
      " '1999-09-02 00:00:00', '1999-09-02 00:00:00', '1999-09-02 00:00:00',\n",
      " '1999-09-02 00:00:00', '1999-09-02 00:00:00', '1999-09-02 00:00:00',\n",
      " '1999-09-02 00:00:00',\n",
      " ...\n",
      " '2017-12-06 00:00:00', '2017-12-06 00:00:00', '2017-12-06 00:00:00',\n",
      " '2017-12-06 00:00:00', '2017-12-06 00:00:00', '2017-12-06 00:00:00',\n",
      " '2017-12-06 00:00:00', '2017-12-06 00:00:00', '2017-12-06 00:00:00',\n",
      " '2017-12-06 00:00:00']\n",
      "Length: 2828, dtype: datetime64[ns]' has dtype incompatible with datetime64[ns, UTC], please explicitly cast to a compatible dtype first.\n",
      "  df['datetime'].fillna(df['Date'], inplace=True)\n",
      "/tmp/ipykernel_1391364/1490424298.py:14: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  yield pd.read_excel(data_f, sheet_name=sheet, parse_dates=[2], na_values=-9999)\n",
      "/tmp/ipykernel_1391364/1490424298.py:47: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '<DatetimeArray>\n",
      "['1999-02-20 00:00:00', '1999-03-15 00:00:00', '1999-03-15 00:00:00',\n",
      " '1999-03-15 00:00:00', '1999-03-15 00:00:00', '1999-03-15 00:00:00',\n",
      " '1999-03-15 00:00:00', '1999-03-18 00:00:00', '1999-03-18 00:00:00',\n",
      " '1999-03-18 00:00:00',\n",
      " ...\n",
      " '2017-12-11 00:00:00', '2017-12-11 00:00:00', '2017-12-11 00:00:00',\n",
      " '2017-12-11 00:00:00', '2017-12-11 00:00:00', '2017-12-11 00:00:00',\n",
      " '2017-12-11 00:00:00', '2017-12-11 00:00:00', '2017-12-11 00:00:00',\n",
      " '2017-12-11 00:00:00']\n",
      "Length: 4797, dtype: datetime64[ns]' has dtype incompatible with datetime64[ns, UTC], please explicitly cast to a compatible dtype first.\n",
      "  df['datetime'].fillna(df['Date'], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13157 Total rows found\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>datetime</th>\n",
       "      <th>depth</th>\n",
       "      <th>value</th>\n",
       "      <th>parameter_id</th>\n",
       "      <th>source_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>QMH002</td>\n",
       "      <td>2001-02-26 17:43:43+00:00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.12</td>\n",
       "      <td>nh4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QMH002</td>\n",
       "      <td>2001-03-20 23:53:35+00:00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.95</td>\n",
       "      <td>nh4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>QMH002</td>\n",
       "      <td>2001-03-20 23:53:35+00:00</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.18</td>\n",
       "      <td>nh4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>QMH002</td>\n",
       "      <td>2001-04-26 00:00:00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.21</td>\n",
       "      <td>nh4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>QMH002</td>\n",
       "      <td>2001-05-21 23:41:22+00:00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.11</td>\n",
       "      <td>nh4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  location_id                   datetime  depth  value parameter_id  source_id\n",
       "0      QMH002  2001-02-26 17:43:43+00:00    0.5   0.12          nh4          1\n",
       "1      QMH002  2001-03-20 23:53:35+00:00    0.5   0.95          nh4          1\n",
       "2      QMH002  2001-03-20 23:53:35+00:00   10.0   1.18          nh4          1\n",
       "3      QMH002        2001-04-26 00:00:00    0.5   0.21          nh4          1\n",
       "4      QMH002  2001-05-21 23:41:22+00:00    0.5   0.11          nh4          1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spreadsheet_re = re.compile(\".*\\\\.xlsx\")\n",
    "\n",
    "processed_data = []\n",
    "\n",
    "def build_df_from_zip(zip_file, data_file):\n",
    "    with zipfile.ZipFile(zip_file) as zf:\n",
    "        with zf.open(data_file) as data_f:\n",
    "            # Some spreadsheets Ecology sends have multiple worksheets we need to parse\n",
    "            wb = load_workbook(data_f)\n",
    "            for sheet in wb.sheetnames[4:-1]:\n",
    "                # Ignore CTD data as we already have this from EIM\n",
    "                if \"CTD\" in sheet:\n",
    "                    continue\n",
    "                yield pd.read_excel(data_f, sheet_name=sheet, parse_dates=[2], na_values=-9999)\n",
    "                \n",
    "def worker(inq, lock, outs):\n",
    "    thread_con = db.connect()\n",
    "    while True:\n",
    "        zip_file, data_file = inq.get()\n",
    "        if zip_file is None:\n",
    "            break\n",
    "        for df in build_df_from_zip(zip_file, data_file):\n",
    "            # Ecology bottle data does not have full timestamps, only the date. To\n",
    "            # remedy this, go off the knowledge that bottles were filled on the\n",
    "            # upcast. Find the corresponding downcast for each sample by matching\n",
    "            # date/station and look for the latest time.\n",
    "            stations = df[\"Station\"].drop_duplicates().to_list()\n",
    "            stations_dict = {}\n",
    "            stations_strs = []\n",
    "            for n,s in enumerate(stations):\n",
    "                stations_strs.append(f':st{n}')\n",
    "                stations_dict[f'st{n}'] = s\n",
    "            # See https://stackoverflow.com/a/53839782\n",
    "            stmt = text(\"SELECT location_id, cast_id, MAX(datetime) AS datetime \"\n",
    "                    f\"FROM obsdata.observations WHERE location_id IN ({', '.join(stations_strs)}) \"\n",
    "                    \"GROUP BY location_id, cast_id\"\n",
    "                   )\n",
    "            downcasts = pd.read_sql(stmt, con=engine, params=stations_dict)\n",
    "            downcasts['date'] = pd.to_datetime(\n",
    "                pd.to_datetime(downcasts['datetime'], utc=True).dt.date)\n",
    "            df = df.merge(downcasts, how='left', left_on=['Station','Date'], right_on=['location_id','date'])\n",
    "            # Prevent unique constraint violations with the CTD data by adding a\n",
    "            # minute to the time\n",
    "            df['datetime'] += pd.Timedelta(1, 'min')\n",
    "            # Fallback if we don't have a matching downcast for particular data:\n",
    "            # just use the raw date\n",
    "            df['datetime'].fillna(df['Date'], inplace=True)\n",
    "\n",
    "            # Drop anything with missing values\n",
    "            df.dropna(subset='Depth_Matching', inplace=True)\n",
    "\n",
    "            # Each spreadsheet has different column names depending on the type of\n",
    "            # data that was given. Go off a lookup table to match column name with\n",
    "            # parameter ID, and perform any necessary unit conversions\n",
    "            for i,row in ecology_column_names.iterrows():\n",
    "                param = row['raw_name']\n",
    "                if param not in df.columns:\n",
    "                    continue\n",
    "                # For each found column, replace values with NaN if the first character\n",
    "                # of the corresponding QC column (the next column over) is not 2_0_3\n",
    "                qc = df[df.columns[df.columns.get_loc(param)+1]]\n",
    "                df.loc[qc != '2_0_3', param] = np.nan\n",
    "\n",
    "                # For each data column, make a view that drops the NaNs, then append\n",
    "                # that view's location_id, datetime, depth, and column value to\n",
    "                # outs. Create a filled parameter_id that corresponds to the\n",
    "                # data column and append it as well.\n",
    "                view = df.dropna(subset=param)\n",
    "\n",
    "                with lock:\n",
    "                    outs['location_id'].extend(view['Station'].to_list())\n",
    "                    outs['datetime'].extend(view['datetime'].to_list())\n",
    "                    outs['depth'].extend(view['Depth_Matching'].to_list())\n",
    "                    outs['value'].extend(view[param].to_list())\n",
    "                    outs['parameter_id'].extend([row['key_name']] * len(view))\n",
    "\n",
    "# For reference/inspiration, see:\n",
    "# https://docs.python.org/3/library/multiprocessing.html#sharing-state-between-processes\n",
    "# https://stackoverflow.com/a/43079667\n",
    "inq = mp.Queue()\n",
    "lock = mp.Lock()\n",
    "with mp.Manager() as manager:\n",
    "    outs = manager.dict()\n",
    "    for k in ['location_id','datetime','depth','value','parameter_id']:\n",
    "        outs[k] = manager.list()\n",
    "    with mp.Pool(initializer=worker, initargs=(inq, lock, outs)) as pool:\n",
    "        for f in glob.glob(ecology_bottle_files):\n",
    "            with zipfile.ZipFile(f) as zf:\n",
    "                files = zf.namelist()\n",
    "                # Find the files in this zip that match the data filename pattern\n",
    "                # See https://stackoverflow.com/a/19502692\n",
    "                data_files = filter(lambda s: s if spreadsheet_re.match(s) else None, files)\n",
    "                for data_file in data_files:\n",
    "                    print(data_file)\n",
    "                    inq.put((f, data_file))\n",
    "        # We're done, signal to the workers to terminate\n",
    "        for _ in range(pool._processes):\n",
    "            inq.put((None, None))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    # Copy the shared memory locally\n",
    "    processed_data = {}\n",
    "    for k in outs:\n",
    "        processed_data[k] = list(outs[k])\n",
    "    processed_data = pd.DataFrame(processed_data)\n",
    "    processed_data['source_id'] = ecology_source_id\n",
    "\n",
    "# There may be overlap in data between the workbooks, so drop any full duplicates\n",
    "processed_data.drop_duplicates(inplace=True)\n",
    "# There are occasional duplicates in the dataset with non-matching values, probably\n",
    "# due to depth mismatches. To prevent mistakes just drop all of them\n",
    "processed_data.drop_duplicates(subset=['datetime','depth','parameter_id','location_id'],\n",
    "                               keep=False, inplace=True)\n",
    "\n",
    "processed_data.to_sql('observations', con=engine, schema='obsdata', index=False, if_exists='append')\n",
    "print(f'{len(processed_data)} Total rows found')\n",
    "processed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d608e838-83a0-4af4-88f0-20d775462d90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:puget_sound_obs_data]",
   "language": "python",
   "name": "conda-env-puget_sound_obs_data-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
